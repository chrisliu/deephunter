{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT Training",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qXj44MjBTZi4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "orig_dir = os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/chrisliu/deephunter.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZUxfvpUTafK",
        "outputId": "1bb1ef8f-ccb6-481d-d2e4-52bc4ed34abd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'deephunter' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(os.path.join(orig_dir, 'deephunter'))"
      ],
      "metadata": {
        "id": "LfaXKcC2TjrG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txhs3KihUnyx",
        "outputId": "b65d7f40-5ace-46c3-beb2-1edbbab87eba"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow==1.15.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (2.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.21.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (3.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (3.1.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (7.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (7.1.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (0.16.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (4.1.2.30)\n",
            "Requirement already satisfied: tinydb in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (4.7.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (1.3.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 15)) (2.23.0)\n",
            "Requirement already satisfied: pyflann in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 16)) (1.6.14)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 17)) (3.0.0)\n",
            "Requirement already satisfied: keras-bert in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 18)) (0.89.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 19)) (3.2.5)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (1.0.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (1.15.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (1.14.1)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (0.37.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (1.0.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (1.46.3)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (3.17.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->-r requirements.txt (line 1)) (3.3.7)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->-r requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->-r requirements.txt (line 1)) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->-r requirements.txt (line 1)) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->-r requirements.txt (line 1)) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->-r requirements.txt (line 1)) (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 4)) (3.0.9)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->-r requirements.txt (line 5)) (1.5.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (3.1.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 13)) (2022.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 15)) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 15)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 15)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 15)) (3.0.4)\n",
            "Requirement already satisfied: keras-transformer==0.40.0 in /usr/local/lib/python3.7/dist-packages (from keras-bert->-r requirements.txt (line 18)) (0.40.0)\n",
            "Requirement already satisfied: keras-pos-embd==0.13.0 in /usr/local/lib/python3.7/dist-packages (from keras-transformer==0.40.0->keras-bert->-r requirements.txt (line 18)) (0.13.0)\n",
            "Requirement already satisfied: keras-embed-sim==0.10.0 in /usr/local/lib/python3.7/dist-packages (from keras-transformer==0.40.0->keras-bert->-r requirements.txt (line 18)) (0.10.0)\n",
            "Requirement already satisfied: keras-position-wise-feed-forward==0.8.0 in /usr/local/lib/python3.7/dist-packages (from keras-transformer==0.40.0->keras-bert->-r requirements.txt (line 18)) (0.8.0)\n",
            "Requirement already satisfied: keras-multi-head==0.29.0 in /usr/local/lib/python3.7/dist-packages (from keras-transformer==0.40.0->keras-bert->-r requirements.txt (line 18)) (0.29.0)\n",
            "Requirement already satisfied: keras-layer-normalization==0.16.0 in /usr/local/lib/python3.7/dist-packages (from keras-transformer==0.40.0->keras-bert->-r requirements.txt (line 18)) (0.16.0)\n",
            "Requirement already satisfied: keras-self-attention==0.51.0 in /usr/local/lib/python3.7/dist-packages (from keras-multi-head==0.29.0->keras-transformer==0.40.0->keras-bert->-r requirements.txt (line 18)) (0.51.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow.keras as keras\n",
        "import io\n",
        "import numpy as np\n",
        "\n",
        "class PretrainedList:\n",
        "    '''Latest copies of pre-trained bert models (as of June 2, 2022)\n",
        "\n",
        "    An updated list is maintained under the official repo:\n",
        "    https://github.com/google-research/bert\n",
        "    '''\n",
        "\n",
        "    # 12-layer, 768-hidden, 12-heads, 110M parameters\n",
        "    base_uncased = \"https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\"\n",
        "    # 24-layer, 1024-hidden, 16-heads, 340M parameters\n",
        "    large_uncased = \"https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip\"\n",
        "    # 12-layer, 768-hidden, 12-heads, 110M parameters\n",
        "    base_cased = \"https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\"\n",
        "    # 24-layer, 1024-hidden, 16-heads, 340M parameters\n",
        "    large_cased = \"https://storage.googleapis.com/bert_models/2018_10_18/cased_L-24_H-1024_A-16.zip\"\n",
        "    # 24-layer, 1024-hidden, 16-heads, 340M parameters\n",
        "    large_uncased_whole = \"https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\"\n",
        "    # 24-layer, 1024-hidden, 16-heads, 340M parameters\n",
        "    large_cased_whole = \"https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip\"\n",
        "    # 104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters\n",
        "    multilingual_uncased = \"https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip\"\n",
        "    # 102 languages, 24-layer, 1024-hidden, 16-heads, 340M parameters\n",
        "    multilingual_cased = \"https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\"\n",
        "    # 12-layer, 768-hidden, 12-heads, 110M parameters\n",
        "    chinese = \"https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\"\n",
        "\n",
        "text_dataset_dir = 'text'\n",
        "\n",
        "def downlaod_imdb():\n",
        "    url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "    dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n",
        "                                      untar=True, cache_subdir=text_dataset_dir)\n",
        "    dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
        "    return dataset_dir\n",
        "\n",
        "def load_imdb(dataset_dir):\n",
        "    def load_data(data_dir):\n",
        "        texts = list()\n",
        "        labels = list()\n",
        "\n",
        "        label_index = {'pos': 1, 'neg': 0}\n",
        "        for label_name, label_val in label_index.items():\n",
        "            label_dir = os.path.join(data_dir, label_name)\n",
        "            for fname in sorted(os.listdir(label_dir)):\n",
        "                fpath = os.path.join(label_dir, fname)\n",
        "                with io.open(fpath, mode='r', encoding='utf8') as ifs:\n",
        "                    texts.append(ifs.read())\n",
        "                    labels.append(label_val)\n",
        "\n",
        "        return texts, labels\n",
        "\n",
        "    train_dir = os.path.join(dataset_dir, 'train')\n",
        "    test_dir = os.path.join(dataset_dir, 'test')\n",
        "\n",
        "    train_texts, train_labels = load_data(train_dir)\n",
        "    test_texts, test_labels = load_data(test_dir)\n",
        "\n",
        "    return train_texts, train_labels, test_texts, test_labels\n",
        "\n",
        "def get_BERTClassifier(url):\n",
        "    model_path = get_pretrained(PretrainedList.base_uncased)\n",
        "    paths = get_checkpoint_paths(model_path)\n",
        "    model = load_trained_model_from_checkpoint(paths.config, paths.checkpoint,\n",
        "                                               training=True)\n",
        "\n",
        "    inputs = model.inputs[:2]\n",
        "    dense = model.get_layer('NSP-Dense').output\n",
        "    outputs =  keras.layers.Dense(units=2, activation='softmax')(dense)\n",
        "\n",
        "    model = keras.models.Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(lr=2e-5),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['sparse_categorical_accuracy']\n",
        "    )\n",
        "\n",
        "    vocab_dict = load_vocabulary(paths.vocab)\n",
        "    tokenizer = Tokenizer(vocab_dict)\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def preprocess_imdb(tokenize, texts):\n",
        "    SEQ_LEN = 512\n",
        "    tokenized = [tokenizer.encode(t, max_len=SEQ_LEN)[0] for t in texts]\n",
        "    tokenized = np.array(tokenized)\n",
        "    return [tokenized, np.zeros_like(tokenized)]"
      ],
      "metadata": {
        "id": "hRwk6vCTUoYg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_bert import (\n",
        "    get_pretrained,\n",
        "    get_checkpoint_paths,\n",
        "    load_trained_model_from_checkpoint,\n",
        "    load_vocabulary,\n",
        "    Tokenizer\n",
        ")\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import shutil\n",
        "import time\n"
      ],
      "metadata": {
        "id": "u-5j2gcqV5Mx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Timer:\n",
        "    def __init__(self):\n",
        "        self.__start = None\n",
        "        self.__end = None\n",
        "        self.__name = \"\"\n",
        "\n",
        "    def start(self, name=\"\"):\n",
        "        if name == \"\":\n",
        "            print(\"Starting timer\")\n",
        "        else:\n",
        "            print(name)\n",
        "\n",
        "        self.__start = time.time()\n",
        "        self.__name = name\n",
        "        return self\n",
        "\n",
        "    def end(self):\n",
        "        self.__end = time.time()\n",
        "        return self\n",
        "\n",
        "    def elapsed(self):\n",
        "        if self.__name == \"\":\n",
        "            return \"Took {:0.2f} seconds\".format(self.__end - self.__start)\n",
        "        else:\n",
        "            return \"{} took {:0.2f} seconds\".format(self.__name,\n",
        "                                                    self.__end - self.__start)\n",
        "        return self\n",
        "\n",
        "timer = Timer()"
      ],
      "metadata": {
        "id": "p1TQzDr6gmIA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the model\n",
        "timer.start(\"Getting model\")\n",
        "model, tokenizer = get_BERTClassifier(PretrainedList.base_cased)\n",
        "timer.end().elapsed()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yi94zsNegpVz",
        "outputId": "a3032ae5-5a5a-4478-845e-2d716a5ae564"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting model\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Getting model took 26.42 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Timer at 0x7fd3d8071b50>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "timer.start(\"Getting IMDB\")\n",
        "dataset_dir = downlaod_imdb()\n",
        "train_texts, train_labels, test_texts, test_labels = load_imdb(dataset_dir)\n",
        "timer.end().elapsed()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoVGI-U2gs4S",
        "outputId": "b5278e2a-1e8f-482d-9706-d8f8d6f78e47"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting IMDB\n",
            "Getting IMDB took 32.86 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Timer at 0x7fd3d8071b50>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "timer.start(\"Preprocessing\")\n",
        "# TODO:TEMP\n",
        "p = np.random.permutation(len(train_texts))\n",
        "processed_train_texts = [train_texts[i] for i in p[:5000]]\n",
        "processed_train_labels = [train_labels[i] for i in p[:5000]]\n",
        "\n",
        "p = np.random.permutation(len(test_texts))\n",
        "processed_test_texts = [test_texts[i] for i in p[:100]]\n",
        "processed_test_labels = [test_labels[i] for i in p[:100]]\n",
        "\n",
        "processed_train_texts = preprocess_imdb(tokenizer, processed_train_texts)\n",
        "processed_test_texts = preprocess_imdb(tokenizer, processed_test_texts)\n",
        "timer.end().elapsed()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "LluR4V2mgvgL",
        "outputId": "8401abd6-e012-45b5-d29c-61e68613084d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Preprocessing took 15.66 seconds'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "timer.start(\"Training\")\n",
        "model.fit(processed_train_texts, processed_train_labels, epochs=3, batch_size=8,\n",
        "          validation_split=0.2, validation_freq=1, shuffle=True)\n",
        "timer.end().elapsed()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bzwn3Wvg5sy",
        "outputId": "70a54686-e65f-49fc-a594-c076ef32c894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training\n",
            "Train on 4000 samples, validate on 1000 samples\n",
            "Epoch 1/3\n",
            "4000/4000 [==============================] - 585s 146ms/sample - loss: 0.2268 - sparse_categorical_accuracy: 0.9137 - val_loss: 0.1695 - val_sparse_categorical_accuracy: 0.9400\n",
            "Epoch 2/3\n",
            "1624/4000 [===========>..................] - ETA: 5:22 - loss: 0.0875 - sparse_categorical_accuracy: 0.9692"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "timer.start(\"Writing out model\")\n",
        "model.save('bert.h5')\n",
        "timer.end().elapsed()"
      ],
      "metadata": {
        "id": "1pJG0tPtg9sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "p0ZCdw0fs386"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}