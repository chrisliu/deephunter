{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Profile",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JQ7EIh-q6wEN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "orig_dir = os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/chrisliu/deephunter.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVOMwHfW62vu",
        "outputId": "ed932956-b068-49b1-8c23-536a8306403f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'deephunter' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(os.path.join(orig_dir, 'deephunter'))"
      ],
      "metadata": {
        "id": "N_9ZL1TO63pM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Z6XLvzz64m3",
        "outputId": "60cae29e-e248-4f01-b405-cf54bc894253"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow==1.15.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (2.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.21.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (3.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (2.10.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (7.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (7.1.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (0.16.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (4.1.2.30)\n",
            "Requirement already satisfied: tinydb in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (4.7.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (1.3.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 15)) (2.23.0)\n",
            "Requirement already satisfied: pyflann in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 16)) (1.6.14)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 17)) (3.0.0)\n",
            "Requirement already satisfied: keras-bert in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 18)) (0.89.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 19)) (3.2.5)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (1.15.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (1.46.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (1.14.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (1.0.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (1.1.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (0.2.2)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (0.37.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (3.17.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->-r requirements.txt (line 1)) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->-r requirements.txt (line 1)) (3.3.7)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->-r requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->-r requirements.txt (line 1)) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->-r requirements.txt (line 1)) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->-r requirements.txt (line 1)) (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 4)) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (3.1.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 13)) (2022.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 15)) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 15)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 15)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 15)) (1.24.3)\n",
            "Requirement already satisfied: keras-transformer==0.40.0 in /usr/local/lib/python3.7/dist-packages (from keras-bert->-r requirements.txt (line 18)) (0.40.0)\n",
            "Requirement already satisfied: keras-multi-head==0.29.0 in /usr/local/lib/python3.7/dist-packages (from keras-transformer==0.40.0->keras-bert->-r requirements.txt (line 18)) (0.29.0)\n",
            "Requirement already satisfied: keras-embed-sim==0.10.0 in /usr/local/lib/python3.7/dist-packages (from keras-transformer==0.40.0->keras-bert->-r requirements.txt (line 18)) (0.10.0)\n",
            "Requirement already satisfied: keras-pos-embd==0.13.0 in /usr/local/lib/python3.7/dist-packages (from keras-transformer==0.40.0->keras-bert->-r requirements.txt (line 18)) (0.13.0)\n",
            "Requirement already satisfied: keras-position-wise-feed-forward==0.8.0 in /usr/local/lib/python3.7/dist-packages (from keras-transformer==0.40.0->keras-bert->-r requirements.txt (line 18)) (0.8.0)\n",
            "Requirement already satisfied: keras-layer-normalization==0.16.0 in /usr/local/lib/python3.7/dist-packages (from keras-transformer==0.40.0->keras-bert->-r requirements.txt (line 18)) (0.16.0)\n",
            "Requirement already satisfied: keras-self-attention==0.51.0 in /usr/local/lib/python3.7/dist-packages (from keras-multi-head==0.29.0->keras-transformer==0.40.0->keras-bert->-r requirements.txt (line 18)) (0.51.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(os.path.join(orig_dir, 'deephunter', 'deephunter'))"
      ],
      "metadata": {
        "id": "E7pO7KBH65oB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "iCGbIv7q7i5p"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_bert import get_custom_objects\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/bert.h5', custom_objects=get_custom_objects())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7g38bZ5571r5",
        "outputId": "7f908e90-69dc-42cf-93b6-5464d84cb45d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.Profile import DNNProfile\n",
        "from keras_bert import (\n",
        "    get_pretrained,\n",
        "    get_checkpoint_paths,\n",
        "    load_trained_model_from_checkpoint,\n",
        "    load_vocabulary,\n",
        "    Tokenizer\n",
        ")\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import io\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "sKxaLZct7drm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Timer:\n",
        "    def __init__(self):\n",
        "        self.__start = None\n",
        "        self.__end = None\n",
        "        self.__name = \"\"\n",
        "\n",
        "    def start(self, name=\"\"):\n",
        "        if name == \"\":\n",
        "            print(\"Starting timer\")\n",
        "        else:\n",
        "            print(name)\n",
        "\n",
        "        self.__start = time.time()\n",
        "        self.__name = name\n",
        "        return self\n",
        "\n",
        "    def end(self):\n",
        "        self.__end = time.time()\n",
        "        return self\n",
        "\n",
        "    def elapsed(self):\n",
        "        if self.__name == \"\":\n",
        "            return \"Took {:0.2f} seconds\".format(self.__end - self.__start)\n",
        "        else:\n",
        "            return \"{} took {:0.2f} seconds\".format(self.__name,\n",
        "                                                    self.__end - self.__start)\n",
        "        return self\n",
        "\n",
        "timer = Timer()"
      ],
      "metadata": {
        "id": "gBmDSUcU-MY3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PretrainedList:\n",
        "    '''Latest copies of pre-trained bert models (as of June 2, 2022)\n",
        "\n",
        "    An updated list is maintained under the official repo:\n",
        "    https://github.com/google-research/bert\n",
        "    '''\n",
        "\n",
        "    # 12-layer, 768-hidden, 12-heads, 110M parameters\n",
        "    base_uncased = \"https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\"\n",
        "    # 24-layer, 1024-hidden, 16-heads, 340M parameters\n",
        "    large_uncased = \"https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip\"\n",
        "    # 12-layer, 768-hidden, 12-heads, 110M parameters\n",
        "    base_cased = \"https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\"\n",
        "    # 24-layer, 1024-hidden, 16-heads, 340M parameters\n",
        "    large_cased = \"https://storage.googleapis.com/bert_models/2018_10_18/cased_L-24_H-1024_A-16.zip\"\n",
        "    # 24-layer, 1024-hidden, 16-heads, 340M parameters\n",
        "    large_uncased_whole = \"https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\"\n",
        "    # 24-layer, 1024-hidden, 16-heads, 340M parameters\n",
        "    large_cased_whole = \"https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip\"\n",
        "    # 104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters\n",
        "    multilingual_uncased = \"https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip\"\n",
        "    # 102 languages, 24-layer, 1024-hidden, 16-heads, 340M parameters\n",
        "    multilingual_cased = \"https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\"\n",
        "    # 12-layer, 768-hidden, 12-heads, 110M parameters\n",
        "    chinese = \"https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\""
      ],
      "metadata": {
        "id": "TC60ggpQ-yGC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = get_pretrained(PretrainedList.base_uncased)\n",
        "paths = get_checkpoint_paths(model_path)\n",
        "vocab_dict = load_vocabulary(paths.vocab)\n",
        "tokenizer = Tokenizer(vocab_dict)"
      ],
      "metadata": {
        "id": "F3V5XHqN-c3r"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_dataset_dir = 'text'\n",
        "def downlaod_imdb():\n",
        "    url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "    dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n",
        "                                      untar=True, cache_subdir=text_dataset_dir)\n",
        "    dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
        "    return dataset_dir\n",
        "\n",
        "def load_imdb(dataset_dir):\n",
        "    def load_data(data_dir):\n",
        "        texts = list()\n",
        "        labels = list()\n",
        "\n",
        "        label_index = {'pos': 1, 'neg': 0}\n",
        "        for label_name, label_val in label_index.items():\n",
        "            label_dir = os.path.join(data_dir, label_name)\n",
        "            for fname in sorted(os.listdir(label_dir)):\n",
        "                fpath = os.path.join(label_dir, fname)\n",
        "                with io.open(fpath, mode='r', encoding='utf8') as ifs:\n",
        "                    texts.append(ifs.read())\n",
        "                    labels.append(label_val)\n",
        "\n",
        "        return texts, labels\n",
        "\n",
        "    train_dir = os.path.join(dataset_dir, 'train')\n",
        "    test_dir = os.path.join(dataset_dir, 'test')\n",
        "\n",
        "    train_texts, train_labels = load_data(train_dir)\n",
        "    test_texts, test_labels = load_data(test_dir)\n",
        "\n",
        "    return train_texts, train_labels, test_texts, test_labels\n",
        "\n",
        "timer.start(\"Getting IMDB\")\n",
        "dataset_dir = downlaod_imdb()\n",
        "train_texts, train_labels, test_texts, test_labels = load_imdb(dataset_dir)\n",
        "timer.end().elapsed()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "45qP03aR-eCA",
        "outputId": "9bef459f-589e-4140-ca7c-c33a7a8cdf8a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting IMDB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Getting IMDB took 43.75 seconds'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_imdb(tokenize, texts):\n",
        "    SEQ_LEN = 512\n",
        "    tokenized = [tokenizer.encode(t, max_len=SEQ_LEN)[0] for t in texts]\n",
        "    tokenized = np.array(tokenized)\n",
        "    return [tokenized, np.zeros_like(tokenized)]\n",
        "\n",
        "timer.start(\"Preprocessing\")\n",
        "# TODO:TEMP\n",
        "p = np.random.permutation(len(train_texts))\n",
        "processed_train_texts = [train_texts[i] for i in p[:500]]\n",
        "processed_train_labels = [train_labels[i] for i in p[:500]]\n",
        "\n",
        "p = np.random.permutation(len(test_texts))\n",
        "processed_test_texts = [test_texts[i] for i in p[:100]]\n",
        "processed_test_labels = [test_labels[i] for i in p[:100]]\n",
        "\n",
        "processed_train_texts = preprocess_imdb(tokenizer, processed_train_texts)\n",
        "processed_test_texts = preprocess_imdb(tokenizer, processed_test_texts)\n",
        "\n",
        "timer.end().elapsed()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "78bRFuSx-3bX",
        "outputId": "9c73914a-f26d-4b5c-da2a-e71ae26bb7ba"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Preprocessing took 2.26 seconds'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import pprint\n",
        "from PIL import Image\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.datasets import mnist,cifar10\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "import os, sys, errno\n",
        "from tensorflow.keras import backend as K"
      ],
      "metadata": {
        "id": "-4qKCkXhANDc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DNNProfile():\n",
        "    def __init__(self, model, exclude_layer=['input', 'flatten'],\n",
        "                 only_layer=[]):\n",
        "        '''\n",
        "        Initialize the model to be tested\n",
        "        :param threshold: threshold to determine if the neuron is activated\n",
        "        :param model_name: ImageNet Model name, can have ('vgg16','vgg19','resnet50')\n",
        "        :param neuron_layer: Only these layers are considered for neuron coverage\n",
        "        '''\n",
        "        self.model = model\n",
        "        self.outputs = []\n",
        "\n",
        "        print('models loaded')\n",
        "\n",
        "        self.layer_to_compute = []\n",
        "        if len(only_layer) > 0:\n",
        "            self.layer_to_compute = only_layer\n",
        "        else:\n",
        "            for layer in self.model.layers:\n",
        "              if all(ex not in layer.name for ex in exclude_layer):\n",
        "                self.layer_to_compute.append(layer.name)\n",
        "\n",
        "        # the layers that are considered in neuron coverage computation\n",
        "        \n",
        "        for layer in self.layer_to_compute:\n",
        "              self.outputs.append(model.get_layer(layer).output)\n",
        "\n",
        "        self.cov_dict = collections.OrderedDict()\n",
        "\n",
        "        print(\"* target layer list:\", self.layer_to_compute)\n",
        "        print(self.outputs)\n",
        "\n",
        "\n",
        "        for layer_name in self.layer_to_compute:\n",
        "            for index in range(self.model.get_layer(layer_name).output_shape[-1]):\n",
        "                # [mean_value_new, squared_mean_value, standard_deviation, lower_bound, upper_bound]\n",
        "                self.cov_dict[(layer_name, index)] = [0.0, 0.0, 0.0, None, None]\n",
        "\n",
        "\n",
        "\n",
        "    def count_layers(self):\n",
        "        return len(self.layer_to_compute)\n",
        "\n",
        "    def count_neurons(self):\n",
        "        return len(self.cov_dict.items())\n",
        "\n",
        "    def count_paras(self):\n",
        "        return self.model.count_params()\n",
        "\n",
        "    def update_coverage(self, input_data):\n",
        "\n",
        "        inp = self.model.input\n",
        "        functor = K.function([inp] + [K.learning_phase()], self.outputs)\n",
        "        outputs = functor([input_data, 0])\n",
        "\n",
        "        for layer_idx, layer_name in enumerate(self.layer_to_compute):\n",
        "            layer_outputs = outputs[layer_idx]\n",
        "\n",
        "            # handle the layer output by each data\n",
        "            # iter is the number of data\n",
        "            for iter, layer_output in enumerate(layer_outputs):\n",
        "                if iter % 1000 == 0:\n",
        "                    print(\"*layer {0}, current/total iteration: {1}/{2}\".format(layer_idx, iter + 1, len(layer_outputs)))\n",
        "\n",
        "                for neuron_idx in range(layer_output.shape[-1]):\n",
        "                    neuron_output = np.mean(layer_output[..., neuron_idx])\n",
        "\n",
        "\n",
        "\n",
        "                    profile_data_list = self.cov_dict[(layer_name, neuron_idx)]\n",
        "\n",
        "                    mean_value = profile_data_list[0]\n",
        "                    squared_mean_value = profile_data_list[1]\n",
        "\n",
        "                    lower_bound = profile_data_list[3]\n",
        "                    upper_bound = profile_data_list[4]\n",
        "\n",
        "                    total_mean_value = mean_value * iter\n",
        "                    total_squared_mean_value = squared_mean_value * iter\n",
        "\n",
        "                    mean_value_new = (neuron_output + total_mean_value) / (iter + 1)\n",
        "                    squared_mean_value = (neuron_output * neuron_output + total_squared_mean_value) / (iter + 1)\n",
        "\n",
        "\n",
        "                    standard_deviation = np.math.sqrt(abs(squared_mean_value - mean_value_new * mean_value_new))\n",
        "\n",
        "                    if (lower_bound is None) and (upper_bound is None):\n",
        "                        lower_bound = neuron_output\n",
        "                        upper_bound = neuron_output\n",
        "                    else:\n",
        "                        if neuron_output < lower_bound:\n",
        "                            lower_bound = neuron_output\n",
        "\n",
        "                        if neuron_output > upper_bound:\n",
        "                            upper_bound = neuron_output\n",
        "\n",
        "                    profile_data_list[0] = mean_value_new\n",
        "                    profile_data_list[1] = squared_mean_value\n",
        "                    profile_data_list[2] = standard_deviation\n",
        "                    profile_data_list[3] = lower_bound\n",
        "                    profile_data_list[4] = upper_bound\n",
        "        \n",
        "                    self.cov_dict[(layer_name, neuron_idx)] = profile_data_list\n",
        "\n",
        "\n",
        "\n",
        "    def dump(self, output_file):\n",
        "\n",
        "        print(\"*profiling neuron size:\", len(self.cov_dict.items()))\n",
        "        for item in self.cov_dict.items():\n",
        "            print(item)\n",
        "        pickle_out = open(output_file, \"wb\")\n",
        "        pickle.dump(self.cov_dict, pickle_out)\n",
        "        pickle_out.close()\n",
        "\n",
        "        print(\"write out profiling coverage results to \", output_file)\n",
        "        print(\"done.\")"
      ],
      "metadata": {
        "id": "IwoU3Cd4_Idp"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profiler = DNNProfile(model, exclude_layer=['Input-Token', 'Input-Segment', 'Embedding-Token'], only_layer=[])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MP6YklVWACsf",
        "outputId": "6755d7fd-0ae7-460f-9a1b-5cec3d9669ca"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models loaded\n",
            "* target layer list: ['Embedding-Segment', 'Embedding-Position', 'Embedding-Dropout', 'Embedding-Norm', 'Encoder-1-MultiHeadSelfAttention', 'Encoder-1-MultiHeadSelfAttention-Dropout', 'Encoder-1-MultiHeadSelfAttention-Add', 'Encoder-1-MultiHeadSelfAttention-Norm', 'Encoder-1-FeedForward', 'Encoder-1-FeedForward-Dropout', 'Encoder-1-FeedForward-Add', 'Encoder-1-FeedForward-Norm', 'Encoder-2-MultiHeadSelfAttention', 'Encoder-2-MultiHeadSelfAttention-Dropout', 'Encoder-2-MultiHeadSelfAttention-Add', 'Encoder-2-MultiHeadSelfAttention-Norm', 'Encoder-2-FeedForward', 'Encoder-2-FeedForward-Dropout', 'Encoder-2-FeedForward-Add', 'Encoder-2-FeedForward-Norm', 'Encoder-3-MultiHeadSelfAttention', 'Encoder-3-MultiHeadSelfAttention-Dropout', 'Encoder-3-MultiHeadSelfAttention-Add', 'Encoder-3-MultiHeadSelfAttention-Norm', 'Encoder-3-FeedForward', 'Encoder-3-FeedForward-Dropout', 'Encoder-3-FeedForward-Add', 'Encoder-3-FeedForward-Norm', 'Encoder-4-MultiHeadSelfAttention', 'Encoder-4-MultiHeadSelfAttention-Dropout', 'Encoder-4-MultiHeadSelfAttention-Add', 'Encoder-4-MultiHeadSelfAttention-Norm', 'Encoder-4-FeedForward', 'Encoder-4-FeedForward-Dropout', 'Encoder-4-FeedForward-Add', 'Encoder-4-FeedForward-Norm', 'Encoder-5-MultiHeadSelfAttention', 'Encoder-5-MultiHeadSelfAttention-Dropout', 'Encoder-5-MultiHeadSelfAttention-Add', 'Encoder-5-MultiHeadSelfAttention-Norm', 'Encoder-5-FeedForward', 'Encoder-5-FeedForward-Dropout', 'Encoder-5-FeedForward-Add', 'Encoder-5-FeedForward-Norm', 'Encoder-6-MultiHeadSelfAttention', 'Encoder-6-MultiHeadSelfAttention-Dropout', 'Encoder-6-MultiHeadSelfAttention-Add', 'Encoder-6-MultiHeadSelfAttention-Norm', 'Encoder-6-FeedForward', 'Encoder-6-FeedForward-Dropout', 'Encoder-6-FeedForward-Add', 'Encoder-6-FeedForward-Norm', 'Encoder-7-MultiHeadSelfAttention', 'Encoder-7-MultiHeadSelfAttention-Dropout', 'Encoder-7-MultiHeadSelfAttention-Add', 'Encoder-7-MultiHeadSelfAttention-Norm', 'Encoder-7-FeedForward', 'Encoder-7-FeedForward-Dropout', 'Encoder-7-FeedForward-Add', 'Encoder-7-FeedForward-Norm', 'Encoder-8-MultiHeadSelfAttention', 'Encoder-8-MultiHeadSelfAttention-Dropout', 'Encoder-8-MultiHeadSelfAttention-Add', 'Encoder-8-MultiHeadSelfAttention-Norm', 'Encoder-8-FeedForward', 'Encoder-8-FeedForward-Dropout', 'Encoder-8-FeedForward-Add', 'Encoder-8-FeedForward-Norm', 'Encoder-9-MultiHeadSelfAttention', 'Encoder-9-MultiHeadSelfAttention-Dropout', 'Encoder-9-MultiHeadSelfAttention-Add', 'Encoder-9-MultiHeadSelfAttention-Norm', 'Encoder-9-FeedForward', 'Encoder-9-FeedForward-Dropout', 'Encoder-9-FeedForward-Add', 'Encoder-9-FeedForward-Norm', 'Encoder-10-MultiHeadSelfAttention', 'Encoder-10-MultiHeadSelfAttention-Dropout', 'Encoder-10-MultiHeadSelfAttention-Add', 'Encoder-10-MultiHeadSelfAttention-Norm', 'Encoder-10-FeedForward', 'Encoder-10-FeedForward-Dropout', 'Encoder-10-FeedForward-Add', 'Encoder-10-FeedForward-Norm', 'Encoder-11-MultiHeadSelfAttention', 'Encoder-11-MultiHeadSelfAttention-Dropout', 'Encoder-11-MultiHeadSelfAttention-Add', 'Encoder-11-MultiHeadSelfAttention-Norm', 'Encoder-11-FeedForward', 'Encoder-11-FeedForward-Dropout', 'Encoder-11-FeedForward-Add', 'Encoder-11-FeedForward-Norm', 'Encoder-12-MultiHeadSelfAttention', 'Encoder-12-MultiHeadSelfAttention-Dropout', 'Encoder-12-MultiHeadSelfAttention-Add', 'Encoder-12-MultiHeadSelfAttention-Norm', 'Encoder-12-FeedForward', 'Encoder-12-FeedForward-Dropout', 'Encoder-12-FeedForward-Add', 'Encoder-12-FeedForward-Norm', 'Extract', 'NSP-Dense', 'dense']\n",
            "[<tf.Tensor 'Embedding-Segment/embedding_lookup/Identity_1:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Embedding-Position/add:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Embedding-Dropout/cond/Merge:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Embedding-Norm/add_1:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-1-MultiHeadSelfAttention/Reshape_25:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-1-MultiHeadSelfAttention-Dropout/cond/Merge:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-1-MultiHeadSelfAttention-Add/add:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-1-MultiHeadSelfAttention-Norm/add_1:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-1-FeedForward/add_3:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-1-FeedForward-Dropout/cond/Merge:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-1-FeedForward-Add/add:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-1-FeedForward-Norm/add_1:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-2-MultiHeadSelfAttention/Reshape_25:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-2-MultiHeadSelfAttention-Dropout/cond/Merge:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-2-MultiHeadSelfAttention-Add/add:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-2-MultiHeadSelfAttention-Norm/add_1:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-2-FeedForward/add_3:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-2-FeedForward-Dropout/cond/Merge:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-2-FeedForward-Add/add:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-2-FeedForward-Norm/add_1:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-3-MultiHeadSelfAttention/Reshape_25:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-3-MultiHeadSelfAttention-Dropout/cond/Merge:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-3-MultiHeadSelfAttention-Add/add:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-3-MultiHeadSelfAttention-Norm/add_1:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-3-FeedForward/add_3:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-3-FeedForward-Dropout/cond/Merge:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-3-FeedForward-Add/add:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-3-FeedForward-Norm/add_1:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-4-MultiHeadSelfAttention/Reshape_25:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-4-MultiHeadSelfAttention-Dropout/cond/Merge:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-4-MultiHeadSelfAttention-Add/add:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-4-MultiHeadSelfAttention-Norm/add_1:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-4-FeedForward/add_3:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-4-FeedForward-Dropout/cond/Merge:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-4-FeedForward-Add/add:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-4-FeedForward-Norm/add_1:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-5-MultiHeadSelfAttention/Reshape_25:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-5-MultiHeadSelfAttention-Dropout/cond/Merge:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-5-MultiHeadSelfAttention-Add/add:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-5-MultiHeadSelfAttention-Norm/add_1:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-5-FeedForward/add_3:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-5-FeedForward-Dropout/cond/Merge:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-5-FeedForward-Add/add:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-5-FeedForward-Norm/add_1:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-6-MultiHeadSelfAttention/Reshape_25:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-6-MultiHeadSelfAttention-Dropout/cond/Merge:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-6-MultiHeadSelfAttention-Add/add:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-6-MultiHeadSelfAttention-Norm/add_1:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-6-FeedForward/add_3:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-6-FeedForward-Dropout/cond/Merge:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-6-FeedForward-Add/add:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-6-FeedForward-Norm/add_1:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-7-MultiHeadSelfAttention/Reshape_25:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-7-MultiHeadSelfAttention-Dropout/cond/Merge:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-7-MultiHeadSelfAttention-Add/add:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-7-MultiHeadSelfAttention-Norm/add_1:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-7-FeedForward/add_3:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-7-FeedForward-Dropout/cond/Merge:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-7-FeedForward-Add/add:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-7-FeedForward-Norm/add_1:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-8-MultiHeadSelfAttention/Reshape_25:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-8-MultiHeadSelfAttention-Dropout/cond/Merge:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-8-MultiHeadSelfAttention-Add/add:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-8-MultiHeadSelfAttention-Norm/add_1:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-8-FeedForward/add_3:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-8-FeedForward-Dropout/cond/Merge:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-8-FeedForward-Add/add:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-8-FeedForward-Norm/add_1:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-9-MultiHeadSelfAttention/Reshape_25:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-9-MultiHeadSelfAttention-Dropout/cond/Merge:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-9-MultiHeadSelfAttention-Add/add:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-9-MultiHeadSelfAttention-Norm/add_1:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-9-FeedForward/add_3:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-9-FeedForward-Dropout/cond/Merge:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-9-FeedForward-Add/add:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-9-FeedForward-Norm/add_1:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-10-MultiHeadSelfAttention/Reshape_25:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-10-MultiHeadSelfAttention-Dropout/cond/Merge:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-10-MultiHeadSelfAttention-Add/add:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-10-MultiHeadSelfAttention-Norm/add_1:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-10-FeedForward/add_3:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-10-FeedForward-Dropout/cond/Merge:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-10-FeedForward-Add/add:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-10-FeedForward-Norm/add_1:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-11-MultiHeadSelfAttention/Reshape_25:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-11-MultiHeadSelfAttention-Dropout/cond/Merge:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-11-MultiHeadSelfAttention-Add/add:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-11-MultiHeadSelfAttention-Norm/add_1:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-11-FeedForward/add_3:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-11-FeedForward-Dropout/cond/Merge:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-11-FeedForward-Add/add:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-11-FeedForward-Norm/add_1:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-12-MultiHeadSelfAttention/Reshape_25:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-12-MultiHeadSelfAttention-Dropout/cond/Merge:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-12-MultiHeadSelfAttention-Add/add:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-12-MultiHeadSelfAttention-Norm/add_1:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-12-FeedForward/add_3:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-12-FeedForward-Dropout/cond/Merge:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-12-FeedForward-Add/add:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Encoder-12-FeedForward-Norm/add_1:0' shape=(?, 512, 768) dtype=float32>, <tf.Tensor 'Extract/strided_slice:0' shape=(?, 768) dtype=float32>, <tf.Tensor 'NSP-Dense/Tanh:0' shape=(?, 768) dtype=float32>, <tf.Tensor 'dense/Softmax:0' shape=(?, 2) dtype=float32>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sub_texts, sub_onehots = processed_train_texts\n",
        "batch_size = 4\n",
        "sub_texts = sub_texts[:batch_size]\n",
        "sub_onehots = sub_onehots[:batch_size]\n",
        "profiler.update_coverage([sub_texts, sub_onehots])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9wSyZTTBZFD",
        "outputId": "b3995b9b-8a7a-4a6d-c8ad-89feea362358"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*layer 0, current/total iteration: 1/4\n",
            "*layer 1, current/total iteration: 1/4\n",
            "*layer 2, current/total iteration: 1/4\n",
            "*layer 3, current/total iteration: 1/4\n",
            "*layer 4, current/total iteration: 1/4\n",
            "*layer 5, current/total iteration: 1/4\n",
            "*layer 6, current/total iteration: 1/4\n",
            "*layer 7, current/total iteration: 1/4\n",
            "*layer 8, current/total iteration: 1/4\n",
            "*layer 9, current/total iteration: 1/4\n",
            "*layer 10, current/total iteration: 1/4\n",
            "*layer 11, current/total iteration: 1/4\n",
            "*layer 12, current/total iteration: 1/4\n",
            "*layer 13, current/total iteration: 1/4\n",
            "*layer 14, current/total iteration: 1/4\n",
            "*layer 15, current/total iteration: 1/4\n",
            "*layer 16, current/total iteration: 1/4\n",
            "*layer 17, current/total iteration: 1/4\n",
            "*layer 18, current/total iteration: 1/4\n",
            "*layer 19, current/total iteration: 1/4\n",
            "*layer 20, current/total iteration: 1/4\n",
            "*layer 21, current/total iteration: 1/4\n",
            "*layer 22, current/total iteration: 1/4\n",
            "*layer 23, current/total iteration: 1/4\n",
            "*layer 24, current/total iteration: 1/4\n",
            "*layer 25, current/total iteration: 1/4\n",
            "*layer 26, current/total iteration: 1/4\n",
            "*layer 27, current/total iteration: 1/4\n",
            "*layer 28, current/total iteration: 1/4\n",
            "*layer 29, current/total iteration: 1/4\n",
            "*layer 30, current/total iteration: 1/4\n",
            "*layer 31, current/total iteration: 1/4\n",
            "*layer 32, current/total iteration: 1/4\n",
            "*layer 33, current/total iteration: 1/4\n",
            "*layer 34, current/total iteration: 1/4\n",
            "*layer 35, current/total iteration: 1/4\n",
            "*layer 36, current/total iteration: 1/4\n",
            "*layer 37, current/total iteration: 1/4\n",
            "*layer 38, current/total iteration: 1/4\n",
            "*layer 39, current/total iteration: 1/4\n",
            "*layer 40, current/total iteration: 1/4\n",
            "*layer 41, current/total iteration: 1/4\n",
            "*layer 42, current/total iteration: 1/4\n",
            "*layer 43, current/total iteration: 1/4\n",
            "*layer 44, current/total iteration: 1/4\n",
            "*layer 45, current/total iteration: 1/4\n",
            "*layer 46, current/total iteration: 1/4\n",
            "*layer 47, current/total iteration: 1/4\n",
            "*layer 48, current/total iteration: 1/4\n",
            "*layer 49, current/total iteration: 1/4\n",
            "*layer 50, current/total iteration: 1/4\n",
            "*layer 51, current/total iteration: 1/4\n",
            "*layer 52, current/total iteration: 1/4\n",
            "*layer 53, current/total iteration: 1/4\n",
            "*layer 54, current/total iteration: 1/4\n",
            "*layer 55, current/total iteration: 1/4\n",
            "*layer 56, current/total iteration: 1/4\n",
            "*layer 57, current/total iteration: 1/4\n",
            "*layer 58, current/total iteration: 1/4\n",
            "*layer 59, current/total iteration: 1/4\n",
            "*layer 60, current/total iteration: 1/4\n",
            "*layer 61, current/total iteration: 1/4\n",
            "*layer 62, current/total iteration: 1/4\n",
            "*layer 63, current/total iteration: 1/4\n",
            "*layer 64, current/total iteration: 1/4\n",
            "*layer 65, current/total iteration: 1/4\n",
            "*layer 66, current/total iteration: 1/4\n",
            "*layer 67, current/total iteration: 1/4\n",
            "*layer 68, current/total iteration: 1/4\n",
            "*layer 69, current/total iteration: 1/4\n",
            "*layer 70, current/total iteration: 1/4\n",
            "*layer 71, current/total iteration: 1/4\n",
            "*layer 72, current/total iteration: 1/4\n",
            "*layer 73, current/total iteration: 1/4\n",
            "*layer 74, current/total iteration: 1/4\n",
            "*layer 75, current/total iteration: 1/4\n",
            "*layer 76, current/total iteration: 1/4\n",
            "*layer 77, current/total iteration: 1/4\n",
            "*layer 78, current/total iteration: 1/4\n",
            "*layer 79, current/total iteration: 1/4\n",
            "*layer 80, current/total iteration: 1/4\n",
            "*layer 81, current/total iteration: 1/4\n",
            "*layer 82, current/total iteration: 1/4\n",
            "*layer 83, current/total iteration: 1/4\n",
            "*layer 84, current/total iteration: 1/4\n",
            "*layer 85, current/total iteration: 1/4\n",
            "*layer 86, current/total iteration: 1/4\n",
            "*layer 87, current/total iteration: 1/4\n",
            "*layer 88, current/total iteration: 1/4\n",
            "*layer 89, current/total iteration: 1/4\n",
            "*layer 90, current/total iteration: 1/4\n",
            "*layer 91, current/total iteration: 1/4\n",
            "*layer 92, current/total iteration: 1/4\n",
            "*layer 93, current/total iteration: 1/4\n",
            "*layer 94, current/total iteration: 1/4\n",
            "*layer 95, current/total iteration: 1/4\n",
            "*layer 96, current/total iteration: 1/4\n",
            "*layer 97, current/total iteration: 1/4\n",
            "*layer 98, current/total iteration: 1/4\n",
            "*layer 99, current/total iteration: 1/4\n",
            "*layer 100, current/total iteration: 1/4\n",
            "*layer 101, current/total iteration: 1/4\n",
            "*layer 102, current/total iteration: 1/4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-OXL6XEKTzS",
        "outputId": "01b4b66d-57dc-46bc-a167-39f88dc94183"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMoIa_rEE3hU",
        "outputId": "ed570d54-ef04-48cc-c288-4dfac391e84c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input-Token (InputLayer)        [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Segment (InputLayer)      [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token (TokenEmbedding [(None, 512, 768), ( 23440896    Input-Token[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Segment (Embedding)   (None, 512, 768)     1536        Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token-Segment (Add)   (None, 512, 768)     0           Embedding-Token[0][0]            \n",
            "                                                                 Embedding-Segment[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Position (PositionEmb (None, 512, 768)     393216      Embedding-Token-Segment[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Dropout (Dropout)     (None, 512, 768)     0           Embedding-Position[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Norm (LayerNormalizat (None, 512, 768)     1536        Embedding-Dropout[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 512, 768)     2362368     Embedding-Norm[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-1-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 512, 768)     0           Embedding-Norm[0][0]             \n",
            "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-1-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-1-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-1-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-2-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-2-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-2-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-3-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-2-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-3-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-3-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-3-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-3-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-4-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-3-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-4-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-4-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-4-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-4-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-5-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-4-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-5-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-5-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-5-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-5-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-6-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-5-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-6-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-6-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-6-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-6-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-7-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-7-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-7-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-7-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-7-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-8-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-7-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-8-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-8-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-8-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-8-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-9-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-8-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-9-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-9-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-9-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 512, 768)     2362368     Encoder-9-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-9-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 512, 768)     1536        Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward (FeedFor (None, 512, 768)     4722432     Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Dropout  (None, 512, 768)     0           Encoder-10-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Add (Add (None, 512, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
            "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Norm (La (None, 512, 768)     1536        Encoder-10-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 512, 768)     2362368     Encoder-10-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-10-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 512, 768)     1536        Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward (FeedFor (None, 512, 768)     4722432     Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Dropout  (None, 512, 768)     0           Encoder-11-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Add (Add (None, 512, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
            "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Norm (La (None, 512, 768)     1536        Encoder-11-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 512, 768)     2362368     Encoder-11-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 512, 768)     1536        Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward (FeedFor (None, 512, 768)     4722432     Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Dropout  (None, 512, 768)     0           Encoder-12-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Add (Add (None, 512, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
            "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Norm (La (None, 512, 768)     1536        Encoder-12-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Extract (Extract)               (None, 768)          0           Encoder-12-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "NSP-Dense (Dense)               (None, 768)          590592      Extract[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 2)            1538        NSP-Dense[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 109,483,778\n",
            "Trainable params: 109,483,778\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /var/colab/app.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTlj-6yeCh0l",
        "outputId": "ada8ed43-db40-4a84-9be9-7680dfd89caa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":30,\"msg\":\"Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret\",\"time\":\"2022-06-03T23:32:59.967Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":30,\"msg\":\"Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret\",\"time\":\"2022-06-03T23:33:00.011Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"    \\t/etc/jupyter/jupyter_notebook_config.json\",\"time\":\"2022-06-03T23:33:00.096Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"    \\t/etc/jupyter/jupyter_notebook_config.json\",\"time\":\"2022-06-03T23:33:00.102Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"    \\t/usr/local/etc/jupyter/jupyter_notebook_config.d/panel-client-jupyter.json\",\"time\":\"2022-06-03T23:33:00.106Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"    \\t/usr/local/etc/jupyter/jupyter_notebook_config.json\",\"time\":\"2022-06-03T23:33:00.106Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"    \\t/usr/etc/jupyter/jupyter_notebook_config.json\",\"time\":\"2022-06-03T23:33:00.110Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"    \\t/usr/local/etc/jupyter/jupyter_notebook_config.d/panel-client-jupyter.json\",\"time\":\"2022-06-03T23:33:00.112Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"    \\t/usr/local/etc/jupyter/jupyter_notebook_config.json\",\"time\":\"2022-06-03T23:33:00.112Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"    \\t/usr/etc/jupyter/jupyter_notebook_config.json\",\"time\":\"2022-06-03T23:33:00.112Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"    \\t/root/.local/etc/jupyter/jupyter_notebook_config.json\",\"time\":\"2022-06-03T23:33:00.114Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"    \\t/root/.jupyter/jupyter_notebook_config.json\",\"time\":\"2022-06-03T23:33:00.116Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"    \\t/root/.local/etc/jupyter/jupyter_notebook_config.json\",\"time\":\"2022-06-03T23:33:00.120Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"    \\t/root/.jupyter/jupyter_notebook_config.json\",\"time\":\"2022-06-03T23:33:00.121Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":30,\"msg\":\"google.colab serverextension initialized.\",\"time\":\"2022-06-03T23:33:00.128Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":30,\"msg\":\"Serving notebooks from local directory: /\",\"time\":\"2022-06-03T23:33:00.129Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":30,\"msg\":\"0 active kernels\",\"time\":\"2022-06-03T23:33:00.130Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":30,\"msg\":\"google.colab serverextension initialized.\",\"time\":\"2022-06-03T23:33:00.132Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":30,\"msg\":\"Serving notebooks from local directory: /\",\"time\":\"2022-06-03T23:33:00.132Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":30,\"msg\":\"0 active kernels\",\"time\":\"2022-06-03T23:33:00.132Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":30,\"msg\":\"The Jupyter Notebook is running at:\",\"time\":\"2022-06-03T23:33:00.132Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":30,\"msg\":\"http://172.28.0.12:9000/\",\"time\":\"2022-06-03T23:33:00.132Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":30,\"msg\":\"Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\",\"time\":\"2022-06-03T23:33:00.133Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":30,\"msg\":\"The Jupyter Notebook is running at:\",\"time\":\"2022-06-03T23:33:00.131Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":30,\"msg\":\"http://172.28.0.2:9000/\",\"time\":\"2022-06-03T23:33:00.133Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":30,\"msg\":\"Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\",\"time\":\"2022-06-03T23:33:00.133Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":30,\"msg\":\"Kernel started: 2facd052-c6e0-499d-8260-3b2ec02b66d3\",\"time\":\"2022-06-03T23:34:04.262Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":30,\"msg\":\"Adapting to protocol v5.1 for kernel 2facd052-c6e0-499d-8260-3b2ec02b66d3\",\"time\":\"2022-06-03T23:34:05.515Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":30,\"msg\":\"Kernel interrupted: 2facd052-c6e0-499d-8260-3b2ec02b66d3\",\"time\":\"2022-06-03T23:34:34.734Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":30,\"msg\":\"Kernel interrupted: 2facd052-c6e0-499d-8260-3b2ec02b66d3\",\"time\":\"2022-06-03T23:34:39.049Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":30,\"msg\":\"Kernel restarted: 2facd052-c6e0-499d-8260-3b2ec02b66d3\",\"time\":\"2022-06-03T23:40:56.093Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-03 23:47:47.216666: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\",\"time\":\"2022-06-03T23:47:47.217Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-03 23:47:47.295623: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\",\"time\":\"2022-06-03T23:47:47.295Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-03 23:47:47.295697: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cedcf896c10c): /proc/driver/nvidia/version does not exist\",\"time\":\"2022-06-03T23:47:47.296Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-03 23:47:47.297289: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\",\"time\":\"2022-06-03T23:47:47.297Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-03 23:47:47.307535: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200160000 Hz\",\"time\":\"2022-06-03T23:47:47.307Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-03 23:47:47.308609: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x235628c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\",\"time\":\"2022-06-03T23:47:47.308Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-03 23:47:47.308655: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\",\"time\":\"2022-06-03T23:47:47.309Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:03:17.492535: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 23592960000 exceeds 10% of system memory.\",\"time\":\"2022-06-04T00:03:17.492Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"tcmalloc: large alloc 23592960000 bytes == 0x9fe86000 @  0x7faf3aee5b6b 0x7faf3af05379 0x7faee11bfee7 0x7faee0fad51f 0x7faee0e7805b 0x7faee0e3da36 0x7faee0e3e8c3 0x7faee0e3ea93 0x7faee9e4225d 0x7faee10f1e0c 0x7faee10e4575 0x7faee11a2021 0x7faee119f718 0x7faf397e56df 0x7faf3a8c76db 0x7faf3ac0061f\",\"time\":\"2022-06-04T00:03:17.610Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":30,\"msg\":\"KernelRestarter: restarting kernel (1/5), keep random ports\",\"time\":\"2022-06-04T00:03:29.092Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"WARNING:root:kernel 2facd052-c6e0-499d-8260-3b2ec02b66d3 restarted\",\"time\":\"2022-06-04T00:03:29.095Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:04:11.870566: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\",\"time\":\"2022-06-04T00:04:11.871Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:04:11.965406: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\",\"time\":\"2022-06-04T00:04:11.965Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:04:11.965493: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cedcf896c10c): /proc/driver/nvidia/version does not exist\",\"time\":\"2022-06-04T00:04:11.966Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:04:11.968556: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\",\"time\":\"2022-06-04T00:04:11.968Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:04:11.993098: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200160000 Hz\",\"time\":\"2022-06-04T00:04:11.993Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:04:11.993511: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x252821c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\",\"time\":\"2022-06-04T00:04:11.994Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:04:11.993554: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\",\"time\":\"2022-06-04T00:04:11.994Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:07:52.137848: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 23592960000 exceeds 10% of system memory.\",\"time\":\"2022-06-04T00:07:52.138Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"tcmalloc: large alloc 23592960000 bytes == 0xa1784000 @  0x7f07da8abb6b 0x7f07da8cb379 0x7f0781406ee7 0x7f07811f451f 0x7f07810bf05b 0x7f0781084a36 0x7f07810858c3 0x7f0781085a93 0x7f078a08925d 0x7f0781338e0c 0x7f078132b575 0x7f07813e9021 0x7f07813e6718 0x7f07d91ab6df 0x7f07da28d6db 0x7f07da5c661f\",\"time\":\"2022-06-04T00:07:52.154Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":30,\"msg\":\"KernelRestarter: restarting kernel (1/5), keep random ports\",\"time\":\"2022-06-04T00:08:02.126Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"WARNING:root:kernel 2facd052-c6e0-499d-8260-3b2ec02b66d3 restarted\",\"time\":\"2022-06-04T00:08:02.127Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:11:07.740368: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\",\"time\":\"2022-06-04T00:11:07.740Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:11:07.809623: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\",\"time\":\"2022-06-04T00:11:07.809Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:11:07.809697: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cedcf896c10c): /proc/driver/nvidia/version does not exist\",\"time\":\"2022-06-04T00:11:07.810Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:11:07.812135: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\",\"time\":\"2022-06-04T00:11:07.812Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:11:07.844111: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200160000 Hz\",\"time\":\"2022-06-04T00:11:07.844Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:11:07.845664: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x239bc1c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\",\"time\":\"2022-06-04T00:11:07.845Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:11:07.845738: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\",\"time\":\"2022-06-04T00:11:07.845Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:35:22.745099: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 23592960000 exceeds 10% of system memory.\",\"time\":\"2022-06-04T00:35:22.745Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"tcmalloc: large alloc 23592960000 bytes == 0x9fe88000 @  0x7fb081d40b6b 0x7fb081d60379 0x7fb02889bee7 0x7fb02868951f 0x7fb02855405b 0x7fb028519a36 0x7fb02851a8c3 0x7fb02851aa93 0x7fb03151e25d 0x7fb0287cde0c 0x7fb0287c0575 0x7fb02887e021 0x7fb02887b718 0x7fb0806406df 0x7fb0817226db 0x7fb081a5b61f\",\"time\":\"2022-06-04T00:35:22.822Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":30,\"msg\":\"KernelRestarter: restarting kernel (1/5), keep random ports\",\"time\":\"2022-06-04T00:35:35.180Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"WARNING:root:kernel 2facd052-c6e0-499d-8260-3b2ec02b66d3 restarted\",\"time\":\"2022-06-04T00:35:35.180Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:38:26.690380: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\",\"time\":\"2022-06-04T00:38:26.690Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:38:26.765193: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\",\"time\":\"2022-06-04T00:38:26.765Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:38:26.765302: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cedcf896c10c): /proc/driver/nvidia/version does not exist\",\"time\":\"2022-06-04T00:38:26.766Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:38:26.766950: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\",\"time\":\"2022-06-04T00:38:26.767Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:38:26.801992: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200160000 Hz\",\"time\":\"2022-06-04T00:38:26.802Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:38:26.803077: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x24b1e1c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\",\"time\":\"2022-06-04T00:38:26.803Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:38:26.803146: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\",\"time\":\"2022-06-04T00:38:26.803Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:41:11.785532: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 23592960000 exceeds 10% of system memory.\",\"time\":\"2022-06-04T00:41:11.785Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"tcmalloc: large alloc 23592960000 bytes == 0xa1022000 @  0x7fbe9cc87b6b 0x7fbe9cca7379 0x7fbe43762ee7 0x7fbe4355051f 0x7fbe4341b05b 0x7fbe433e0a36 0x7fbe433e18c3 0x7fbe433e1a93 0x7fbe4c3e525d 0x7fbe43694e0c 0x7fbe43687575 0x7fbe43745021 0x7fbe43742718 0x7fbe9b5876df 0x7fbe9c6696db 0x7fbe9c9a261f\",\"time\":\"2022-06-04T00:41:11.807Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":30,\"msg\":\"KernelRestarter: restarting kernel (1/5), keep random ports\",\"time\":\"2022-06-04T00:41:20.206Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"WARNING:root:kernel 2facd052-c6e0-499d-8260-3b2ec02b66d3 restarted\",\"time\":\"2022-06-04T00:41:20.207Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"tcmalloc: large alloc 27894276096 bytes == 0x3b4a000 @  0x7f3bac108001 0x4a3a4a 0x5a7748 0x5a7826 0x51286f 0x549576 0x604173 0x62a809 0x59358d 0x515244 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549e0e 0x4bca8a 0x59c019 0x595ef6 0x5134a6 0x549e0e 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x511e2c 0x593dd7 0x5118f8\",\"time\":\"2022-06-04T00:42:06.278Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":30,\"msg\":\"KernelRestarter: restarting kernel (1/5), keep random ports\",\"time\":\"2022-06-04T00:42:17.283Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"WARNING:root:kernel 2facd052-c6e0-499d-8260-3b2ec02b66d3 restarted\",\"time\":\"2022-06-04T00:42:17.284Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"tcmalloc: large alloc 27894276096 bytes == 0x287a000 @  0x7fe450e64001 0x4a3a4a 0x5a7748 0x5a7826 0x51286f 0x549576 0x604173 0x62a809 0x59358d 0x515244 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549e0e 0x4bca8a 0x59c019 0x595ef6 0x5134a6 0x549e0e 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x511e2c 0x593dd7 0x5118f8\",\"time\":\"2022-06-04T00:42:32.874Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":30,\"msg\":\"KernelRestarter: restarting kernel (1/5), keep random ports\",\"time\":\"2022-06-04T00:42:44.295Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"WARNING:root:kernel 2facd052-c6e0-499d-8260-3b2ec02b66d3 restarted\",\"time\":\"2022-06-04T00:42:44.296Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:45:50.999219: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\",\"time\":\"2022-06-04T00:45:50.999Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:45:51.077082: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\",\"time\":\"2022-06-04T00:45:51.077Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:45:51.077192: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cedcf896c10c): /proc/driver/nvidia/version does not exist\",\"time\":\"2022-06-04T00:45:51.078Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:45:51.079138: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\",\"time\":\"2022-06-04T00:45:51.079Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:45:51.107056: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200160000 Hz\",\"time\":\"2022-06-04T00:45:51.107Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:45:51.108508: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x236361c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\",\"time\":\"2022-06-04T00:45:51.108Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 00:45:51.108575: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\",\"time\":\"2022-06-04T00:45:51.108Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"2022-06-04 01:02:30.797098: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 23592960000 exceeds 10% of system memory.\",\"time\":\"2022-06-04T01:02:30.797Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"tcmalloc: large alloc 23592960000 bytes == 0x9fb46000 @  0x7f592d4d0b6b 0x7f592d4f0379 0x7f58d3fabee7 0x7f58d3d9951f 0x7f58d3c6405b 0x7f58d3c29a36 0x7f58d3c2a8c3 0x7f58d3c2aa93 0x7f58dcc2e25d 0x7f58d3edde0c 0x7f58d3ed0575 0x7f58d3f8e021 0x7f58d3f8b718 0x7f592bdd06df 0x7f592ceb26db 0x7f592d1eb61f\",\"time\":\"2022-06-04T01:02:30.915Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":30,\"msg\":\"KernelRestarter: restarting kernel (1/5), keep random ports\",\"time\":\"2022-06-04T01:02:44.307Z\",\"v\":0}\n",
            "{\"pid\":7,\"type\":\"jupyter\",\"level\":40,\"msg\":\"WARNING:root:kernel 2facd052-c6e0-499d-8260-3b2ec02b66d3 restarted\",\"time\":\"2022-06-04T01:02:44.308Z\",\"v\":0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qjpQ9CdvKVUh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}